# Berta AI Scribe - Docker Compose Environment Variables
# Copy this file to .env and fill in your values:
#   cp .env.example .env

# =============================================================================
# REQUIRED - Must be set before running
# =============================================================================

# JWT Secret for token signing (generate with: openssl rand -base64 32)
ACCESS_TOKEN_SECRET=your_generated_secret_here

# Google OAuth Credentials (from Google Cloud Console)
# See README.md "Setting up Google OAuth" section for setup instructions
GOOGLE_CLIENT_ID=your_google_client_id
GOOGLE_CLIENT_SECRET=your_google_client_secret

# =============================================================================
# AI SERVICE CONFIGURATION - Choose ONE option
# =============================================================================

# -----------------------------------------------------------------------------
# Option 1: OpenAI (Easiest - Recommended for getting started)
# -----------------------------------------------------------------------------
TRANSCRIPTION_SERVICE=OpenAI Whisper
GENERATIVE_AI_SERVICE=OpenAI
DEFAULT_NOTE_GENERATION_MODEL=gpt-4o
LABEL_MODEL=gpt-4o
OPENAI_API_KEY=your_openai_api_key

# -----------------------------------------------------------------------------
# Option 2: Ollama (Local/Offline)
# Requires: Run 'ollama serve' and 'ollama pull llama3.1:8b' on host machine
# -----------------------------------------------------------------------------
# TRANSCRIPTION_SERVICE=WhisperX
# GENERATIVE_AI_SERVICE=Ollama
# DEFAULT_NOTE_GENERATION_MODEL=llama3.1:8b
# LABEL_MODEL=llama3.1:8b
# WHISPERX_DEVICE=cpu

# -----------------------------------------------------------------------------
# Option 3: VLLM (Local GPU - High Performance)
# Requires: NVIDIA GPU, VLLM server running, Hugging Face token
# -----------------------------------------------------------------------------
# TRANSCRIPTION_SERVICE=WhisperX
# GENERATIVE_AI_SERVICE=VLLM
# DEFAULT_NOTE_GENERATION_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct
# LABEL_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct
# VLLM_MODEL_NAME=meta-llama/Meta-Llama-3.1-70B-Instruct
# VLLM_SERVER_NAME=localhost
# VLLM_SERVER_PORT=8080
# HUGGINGFACE_TOKEN=your_huggingface_token
# WHISPERX_DEVICE=cuda

# -----------------------------------------------------------------------------
# Option 4: LlamaCpp (Local GPU - DGX Spark / High Performance)
# Requires: llama-server running on host
# -----------------------------------------------------------------------------
# TRANSCRIPTION_SERVICE=WhisperX
# GENERATIVE_AI_SERVICE=LlamaCpp
# DEFAULT_NOTE_GENERATION_MODEL=Llama-3.3-70B-Instruct-Q4_K_M.gguf
# LABEL_MODEL=Llama-3.3-70B-Instruct-Q4_K_M.gguf
# LLAMA_CPP_SERVER_URL=http://host.docker.internal:8080
# WHISPERX_DEVICE=cuda

# =============================================================================
# OPTIONAL SETTINGS
# =============================================================================

# WhisperX device: cpu (default) or cuda (if NVIDIA GPU available)
# WHISPERX_DEVICE=cpu

# Ollama host (only needed if Ollama runs on different host)
# OLLAMA_HOST=http://host.docker.internal:11434
